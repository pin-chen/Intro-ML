# HW3

## **Part. 1, Coding (70%)**:

#### 1. (5%) Gini Index or Entropy is often used for measuring the â€œbestâ€ splitting of the data. Please compute the Entropy and Gini Index of this array `np.array([1,2,1,1,1,1,2,2,1,1,2])` by the formula below. (More details on [page 5 of the hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15), 1 and 2 represent class1 and class 2, respectively)

![image-20221115191220201](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115191220201.png)

#### 2. (10%) Implement the Decision Tree algorithm[ (CART, Classification and Regression Trees)](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) and train the model by the given arguments, and print the accuracy score on the test data. You should implement **two arguments** for the Decision Tree algorithm, 

1) **Criterion**: The function to measure the quality of a split. Your model should support â€œginiâ€ for the Gini impurity and â€œentropyâ€ for the information gain. 
2) **Max_depth**: The maximum depth of the tree. If Max_depth=None, then nodes are expanded until all leaves are pure. Max_depth=1 equals split data once

##### 2.1. Using Criterion=â€˜giniâ€™, showing the accuracy score of test data by Max_depth=3 and Max_depth=10, respectively.

![image-20221115233442289](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115233442289.png)

##### 2.2. Using Max_depth=3, showing the accuracy score of test data by Criterion=â€˜giniâ€™ and Criterion=â€™entropyâ€™, respectively.

![image-20221115233519347](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115233519347.png)

#### 3. (5%) Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can use the model from Question 2.1, max_depth=10. (You can use simply counting to get the feature importance instead of the formula in the reference, more details on the sample code. **Matplotlib** is allowed to be used)

![image-20221115234129666](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115234129666.png)

#### 4. (15%) Implement the AdaBoost algorithm by using the CART you just implemented from question 2. You should implement **one argument** for the AdaBoost. 

1) **N_estimators**: The number of trees in the forest. 

##### 4.1. Showing the accuracy score of test data by n_estimators=10 and n_estimators=100, respectively.

![image-20221115233746145](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115233746145.png)

#### 5. (15%) Implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement **three arguments** for the Random Forest.

1. **N_estimators**: The number of trees in the forest. 
2. **Max_features**: The number of features to consider when looking for the best split
3. **Bootstrap**: Whether bootstrap samples are used when building trees

##### 5.1. Using Criterion=â€˜giniâ€™, Max_depth=None, Max_features=sqrt(n_features), Bootstrap=True**,** showing the accuracy score of test data by n_estimators=10 and n_estimators=100, respectively.

![image-20221115191442140](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115191442140.png)

##### 5.2. Using Criterion=â€˜giniâ€™, Max_depth=None, N_estimators=10, Bootstrap=True,  showing the accuracy score of test data by Max_features=sqrt(n_features) and Max_features=n_features, respectively.

![image-20221115191455582](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115191455582.png)

#### 6. (20%) Tune the hyperparameter, perform feature engineering or implement more powerful ensemble methods to get a higher accuracy score. Please note that only the ensemble method can be used. The neural network method is not allowed.

![image-20221115191521486](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115191521486.png)

## **Part. 2, Questions (30%):**

#### 1. Why does a decision tree have a tendency to overfit to the training set? Is it possible for a decision tree to reach a 100% accuracy in the training set? please explain. List and describe at least 3 strategies we can use to reduce the risk of overfitting of a decision tree.

å› ç‚º decision tree ä¸€ç›´é•·ä¸‹å»ï¼Œæœƒé‡å° train data çš„è³‡æ–™é€²è¡Œç´°åˆ†ï¼Œç›´åˆ°æ¯å€‹é»éƒ½æ˜¯ç›¸åŒé¡åˆ¥çš„è³‡æ–™å°è‡´æ¯å€‹ leaf node çš„è³‡æ–™éåº¦ç´°åˆ†è€Œå°è‡´ overfitï¼Œä¹Ÿå› æ­¤æ‹¿ train data é€²è¡Œ predict çš„è©±å¯ä»¥åˆ°é” 100%ã€‚

1. å¢åŠ  trainning data: æœ‰å¤ æ¨£çš„æ•¸æ“šï¼Œå°±ä¸æœƒå°è‡´ decision tree éåº¦åå‘æŸä¸€ç¨®é¡çš„æ•¸æ“š
2. ç•¶ node çš„ data æ•¸éå°å°±ä¸å† split: é¿å… decision tree æ·±åº¦åˆ°å¤ªæ·±ï¼Œå°‡æ¯å€‹ leaf node éƒ½ç´°åˆ†æˆæ¥µå°‘é‡çš„æ•¸æ“š
3. äº¤å‰é©—è­‰: å°‡è³‡æ–™åˆ‡åˆ†æˆ k ä»½ï¼Œç”¨ k - 1 ä»½ data é€²è¡Œ train ï¼Œå†ç”¨å‰©ä¸‹é‚£ä»½é€²è¡Œ test ï¼Œé€²è¡Œ k æ¬¡ï¼Œè©•ä¼°æ˜¯å¦ overfit ï¼Œå†é‡å°è¶…åƒæ•¸é€²è¡Œèª¿æ•´ã€‚

#### 2. This part consists of three True/False questions. Answer True/False for each question and briefly explain your answer.

##### a. In AdaBoost, weights of the misclassified examples go up by the same multiplicative factor.

True. $D_{t+1}(i)=\frac{D_t(i)\exp(-a_t y_i h_t(x_i))}{Z_t}$ é€™å€‹ weak classifier predict éŒ¯çš„è³‡æ–™ $y_ih_t(x_i)$ æœƒç‚º $-1$, æ­£ç¢ºçš„è³‡æ–™ç‚º $1$ã€‚å› æ­¤åŒè¼ªçš„ weight ï¼Œæ­£ç¢ºçš„éƒ½ä¹˜ $\exp(-a_i)$ ï¼ŒéŒ¯èª¤çš„éƒ½ä¹˜ $\exp(a_i)$ ï¼Œæœ€å¾Œå† normalizationã€‚Hence weights of the misclassified examples go up by the same multiplicative factor.

##### b. In AdaBoost, weighted training error $ğœº_t$ of the $t_\text{th}$ weak classifier on training data with weights $D_t$ tends to increase as a function of $t$.

True. å¢åŠ éŒ¯èª¤è³‡æ–™çš„æ¬Šé‡ï¼Œæœƒå°è‡´éŒ¯èª¤è³‡æ–™ä¸€ç›´è¢«é‡è¦–ï¼Œä½†é€™äº›è³‡æ–™æœ¬èº«å°±æ˜¯å¾ˆé›£è¢«åˆ†é¡çš„è³‡æ–™ï¼Œæœƒå°è‡´ä¸€ç›´åˆ†ä¸€ç›´éŒ¯ï¼Œå› è€Œå¢åŠ æ¬Šé‡ errorã€‚

##### c. AdaBoost will eventually give zero training error regardless of the type of weak classifier it uses, provided enough iterations are performed.

False. weak classifier éœ€è¦æ¯” random äº‚çŒœä¾†å¾—å¥½ã€‚

#### 3. Consider a data set comprising $400$ data points from class $C_1$ and $400$ data points from class $C_2$. Suppose that a tree model A splits these into $(200, 400)$ at the first leaf node and $(200, 0)$ at the second leaf node, where $(n, m)$ denotes that $n$ points are assigned to $C_1$ and m points are assigned to $C_2$. Similarly, suppose that a second tree model B splits them into $(300, 100)$ and $(100, 300)$. **Evaluate the** **misclassification rates** **for the two trees and hence show that they are equal**. Similarly, **evaluate the** **cross-entropy** $Entropy=-\sum_{k=1}^K p_k log_2 p_k$ and Gini index $Gini=1-\sum_{k=1}^K p_k^2$ for **the two trees**. Define $p_k$ to be the proportion of data points in region R assigned to class $k$, where $k = 1, \dots , K$.

![image-20221115220151051](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20221115220151051.png)

å»ºç«‹ decision tree æ™‚ï¼Œleaf node çš„ class æœƒè¢«åˆ¤å®šç‚ºè¼ƒå¤šè³‡æ–™çš„ class å› æ­¤ $x_1, x_4$ ç‚º $C_2$ ï¼Œ $x_2, x_3$ ç‚º $C_1$ ï¼Œè€Œå°æ–¼ A tree è€Œè¨€æœ‰ 200 å€‹ $C_1$ data è¢«èª¤åˆ†åˆ° $x_1$ ï¼Œå°æ–¼ B tree è€Œè¨€æœ‰ 100  å€‹ $C_2$ data èª¤åˆ†åˆ° $x_3$ ï¼Œ 100  å€‹ $C_1$ data èª¤åˆ†åˆ° $x_4$ ï¼Œå…©å€‹ tree çš„ misclassification rates éƒ½æ˜¯ $0.25$ ã€‚

$x_1:$

â€‹	$Entropt=-(400/600)\log_2(400/600)-(200/600)\log_2(200/600)=0.918$

â€‹	$Gini=1-(400/600)^2-(200/600)^2=4/9=0.444$

$x_2:$

â€‹	$Entropt=-(200/200)\log_2(200/200)-(0/200)\log_2(0/200)=0$

â€‹	$Gini=1-(200/200)^2-(0/200)^2=0$

$x_3:$

â€‹	$Entropt=-(300/400)\log_2(300/400)-(100/400)\log_2(100/400)=0.811$

â€‹	$Gini=1-(300/400)^2-(100/400)^2=3/8=0.375$

$x_4:$

â€‹	$Entropt=-(100/400)\log_2(100/400)-(300/400)\log_2(300/400)=0.811$

â€‹	$Gini=1-(100/400)^2-(300/400)^2=3/8=0.375$